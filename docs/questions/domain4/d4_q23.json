{
  "id": "d4_q23",
  "type": "single",
  "text": "次のシナリオを考えてください： 「医療機関がMLモデルの説明可能性を確保したい」 最も適切な実装アプローチはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ブラックボックスモデルのみを使用"
    },
    {
      "label": "B",
      "text": "SHAP値、特徴重要度、決定木の可視化を組み合わせる"
    },
    {
      "label": "C",
      "text": "説明は不要と判断する"
    },
    {
      "label": "D",
      "text": "より複雑なモデルを使用する"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "<h5>詳細解説</h5><p>正解はB: SHAP値、特徴重要度、決定木の可視化を組み合わせるです。</p><p>医療分野では、AIの判断根拠を説明できることが法的・倫理的に重要です。説明可能AI（XAI）技術を活用することで、医師や患者に対して透明性のある意思決定支援を提供できます。</p><h5>各選択肢の解説</h5><p>A) ブラックボックスモデルのみを使用 - 医療分野では不適切です。診断や治療の判断には根拠の説明が必要で、規制要件（FDA、CE-MDR）でも説明可能性が求められます。</p><h5>B) SHAP値、特徴重要度、決定木の可視化を組み合わせる（正解）- 包括的な説明可能性アプローチ</h5><ul><li>SHAP（SHapley Additive exPlanations）：</li><li>個々の予測に対する特徴の貢献度</li><li>グローバルとローカルな説明</li><li>視覚的なウォーターフォール図</li><li>特徴重要度：</li><li>モデル全体での変数の影響力</li><li>臨床的な妥当性の検証</li><li>医師の知見との整合性確認</li><li>決定木の可視化：</li><li>意思決定プロセスの明示</li><li>IF-THENルールの抽出</li><li>臨床ガイドラインとの対応</li></ul><p>C) 説明は不要と判断する - 医療分野では許されません。患者の生命に関わる判断には説明責任が伴います。</p><p>D) より複雑なモデルを使用する - 複雑性は説明可能性を低下させます。性能と説明可能性のバランスが重要です。</p><h5>実践例：糖尿病リスク予測システムの実装</h5><h5>1. モデル選択</h5><ul><li>主モデル：勾配ブースティング（高精度）</li><li>補助モデル：決定木（説明用）</li><li>アンサンブル：両方の利点を活用</li></ul><h5>2. 説明可能性の実装</h5><p>```python</p><p># SHAP値の計算</p><p>explainer = shap.TreeExplainer(model)</p><p>shap_values = explainer.shap_values(X_test)</p><p># 個別患者の説明</p><p>shap.force_plot(explainer.expected_value,</p><p>shap_values[0], X_test[0])</p><p>```</p><h5>3. 医師向けダッシュボード</h5><ul><li>リスクスコアと信頼区間</li><li>主要なリスク要因のハイライト</li><li>類似症例の参照</li><li>介入による影響のシミュレーション</li></ul><h5>4. 規制対応</h5><ul><li>FDA 510(k)申請での説明文書</li><li>臨床試験での検証結果</li><li>監査証跡の保持</li></ul><p>このアプローチにより、高精度な予測と臨床的な解釈可能性を両立し、医師の信頼を獲得できます。</p><h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) ブラックボックスモデルのみを使用:</strong> 学習率は慎重に選択する必要があり、高すぎると発散、低すぎると収束が遅くなります。</li><li><strong>D) より複雑なモデルを使用する:</strong> エポック数の無制限な増加は過学習につながり、早期停止などの手法が推奨されます。</li></ul>",
  "resources": []
}