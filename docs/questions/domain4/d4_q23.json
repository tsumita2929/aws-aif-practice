{
  "id": "d4_q23",
  "type": "single",
  "text": "次のシナリオを考えてください： 「医療機関がMLモデルの説明可能性を確保したい」 最も適切な実装アプローチはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ブラックボックスモデルのみを使用"
    },
    {
      "label": "B",
      "text": "SHAP値、特徴重要度、決定木の可視化を組み合わせる"
    },
    {
      "label": "C",
      "text": "説明は不要と判断する"
    },
    {
      "label": "D",
      "text": "より複雑なモデルを使用する"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "正解はB: SHAP値、特徴重要度、決定木の可視化を組み合わせるです。\n\n医療分野では、AIの判断根拠を説明できることが法的・倫理的に重要です。説明可能AI（XAI）技術を活用することで、医師や患者に対して透明性のある意思決定支援を提供できます。\n\n各選択肢の解説：\nA) ブラックボックスモデルのみを使用 - 医療分野では不適切です。診断や治療の判断には根拠の説明が必要で、規制要件（FDA、CE-MDR）でも説明可能性が求められます。\n\nB) SHAP値、特徴重要度、決定木の可視化を組み合わせる（正解）- 包括的な説明可能性アプローチ：\n  ・SHAP（SHapley Additive exPlanations）：\n    - 個々の予測に対する特徴の貢献度\n    - グローバルとローカルな説明\n    - 視覚的なウォーターフォール図\n  ・特徴重要度：\n    - モデル全体での変数の影響力\n    - 臨床的な妥当性の検証\n    - 医師の知見との整合性確認\n  ・決定木の可視化：\n    - 意思決定プロセスの明示\n    - IF-THENルールの抽出\n    - 臨床ガイドラインとの対応\n\nC) 説明は不要と判断する - 医療分野では許されません。患者の生命に関わる判断には説明責任が伴います。\n\nD) より複雑なモデルを使用する - 複雑性は説明可能性を低下させます。性能と説明可能性のバランスが重要です。\n\n実践例：糖尿病リスク予測システムの実装：\n1. モデル選択：\n   - 主モデル：勾配ブースティング（高精度）\n   - 補助モデル：決定木（説明用）\n   - アンサンブル：両方の利点を活用\n\n2. 説明可能性の実装：\n   ```python\n   # SHAP値の計算\n   explainer = shap.TreeExplainer(model)\n   shap_values = explainer.shap_values(X_test)\n   \n   # 個別患者の説明\n   shap.force_plot(explainer.expected_value, \n                   shap_values[0], X_test[0])\n   ```\n\n3. 医師向けダッシュボード：\n   - リスクスコアと信頼区間\n   - 主要なリスク要因のハイライト\n   - 類似症例の参照\n   - 介入による影響のシミュレーション\n\n4. 規制対応：\n   - FDA 510(k)申請での説明文書\n   - 臨床試験での検証結果\n   - 監査証跡の保持\n\nこのアプローチにより、高精度な予測と臨床的な解釈可能性を両立し、医師の信頼を獲得できます。",
  "resources": []
}