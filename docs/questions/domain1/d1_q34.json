{
  "id": "d1_q34",
  "type": "single",
  "text": "自然言語処理における「埋め込み（Embedding）」の目的は何ですか？",
  "choices": [
    {
      "label": "A",
      "text": "テキストを削除する"
    },
    {
      "label": "B",
      "text": "単語や文を密な数値ベクトルに変換し、意味的な関係を捉える"
    },
    {
      "label": "C",
      "text": "文章を長くする"
    },
    {
      "label": "D",
      "text": "文法をチェックする"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>埋め込み（Embedding）は、高次元のカテゴリカルデータ（単語、文など）を低次元の密な数値ベクトルに変換する技術で、意味的な関係を数値的に表現できます。</p>\n                \n                <h5>埋め込みの主要な特徴</h5>\n                <ul>\n                    <li><strong>次元削減：</strong>\n                        <ul>\n                            <li>One-hot encoding: 語彙数次元（例：50,000次元）</li>\n                            <li>Embedding: 固定次元（例：300次元）</li>\n                            <li>計算効率の大幅な向上</li>\n                        </ul>\n                    </li>\n                    <li><strong>意味的関係の捕捉：</strong>\n                        <ul>\n                            <li>類似単語は近いベクトル</li>\n                            <li>ベクトル演算で意味的操作が可能</li>\n                            <li>例：king - man + woman ≈ queen</li>\n                        </ul>\n                    </li>\n                </ul>\n                \n                <h5>主要な埋め込み手法</h5>\n                <ul>\n                    <li><strong>Word2Vec：</strong>\n                        <ul>\n                            <li>CBOW（Continuous Bag of Words）</li>\n                            <li>Skip-gram</li>\n                            <li>文脈から単語の意味を学習</li>\n                        </ul>\n                    </li>\n                    <li><strong>GloVe（Global Vectors）：</strong>\n                        <ul>\n                            <li>共起行列の統計情報を活用</li>\n                            <li>グローバルな文脈を考慮</li>\n                        </ul>\n                    </li>\n                    <li><strong>BERT/GPT埋め込み：</strong>\n                        <ul>\n                            <li>文脈依存の動的埋め込み</li>\n                            <li>同じ単語でも文脈により異なるベクトル</li>\n                        </ul>\n                    </li>\n                </ul>\n                \n                <h5>実装例（Python）</h5>\n                <pre><code># TensorFlow/Kerasでの埋め込み層\nfrom tensorflow.keras.layers import Embedding\n\n# 語彙サイズ10000、埋め込み次元128\nembedding_layer = Embedding(\n    input_dim=10000,      # 語彙サイズ\n    output_dim=128,       # 埋め込み次元\n    input_length=100      # シーケンス長\n)\n\n# 事前学習済み埋め込みの使用\nimport gensim.downloader as api\nword_vectors = api.load(\"glove-wiki-gigaword-100\")\nvector = word_vectors['computer']  # 100次元ベクトル</code></pre>\n                \n                <h5>なぜ他の選択肢が誤りか</h5>\n                <ul>\n                    <li><strong>A（テキストを削除）：</strong> 埋め込みは変換であり、削除ではない</li>\n                    <li><strong>C（文章を長くする）：</strong> 長さの変更ではなく、表現形式の変換</li>\n                    <li><strong>D（文法チェック）：</strong> 文法解析とは異なる概念</li>\n                </ul>\n                \n                <h5>AWSでの活用</h5>\n                <ul>\n                    <li><strong>Amazon Comprehend：</strong> 内部的に埋め込みを使用してテキスト分析</li>\n                    <li><strong>Amazon SageMaker：</strong> カスタム埋め込みモデルの訓練</li>\n                    <li><strong>Amazon Bedrock：</strong> LLMの埋め込み機能を活用</li>\n                </ul>\n            <h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) テキストを削除する:</strong> 埋め込みは情報を保持しながら変換する技術です。削除ではなく、テキストの意味を数値ベクトルに変換して機械学習で扱えるようにします。</li><li><strong>C) 文章を長くする:</strong> 埋め込みは文章の長さを変更するものではありません。むしろ可変長のテキストを固定次元のベクトルに変換する技術です。</li><li><strong>D) 文法をチェックする:</strong> 文法チェックは構文解析の領域です。埋め込みは意味的な類似性を捉えることが目的で、文法の正しさとは別の概念です。</li></ul>",
  "resources": []
}