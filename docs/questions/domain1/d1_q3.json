{
  "id": "d1_q3",
  "type": "single",
  "text": "ニューラルネットワークの「過学習（オーバーフィッティング）」を防ぐ手法として適切でないものはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ドロップアウト"
    },
    {
      "label": "B",
      "text": "早期停止（Early Stopping）"
    },
    {
      "label": "C",
      "text": "学習率の増加"
    },
    {
      "label": "D",
      "text": "正則化（Regularization）"
    }
  ],
  "correct": [
    2
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>学習率の増加は過学習を悪化させる可能性があります。大きな学習率は不安定な学習を引き起こします。</p>\n                <h5>過学習を防ぐ手法</h5>\n                <ul>\n                    <li><strong>ドロップアウト:</strong> ランダムにニューロンを無効化し、モデルの汎化性能を向上</li>\n                    <li><strong>早期停止:</strong> 検証損失が増加し始めたら学習を停止</li>\n                    <li><strong>正則化:</strong> L1/L2正則化により重みの大きさを制限</li>\n                </ul>\n            <h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) ドロップアウト:</strong> ドロップアウトは過学習を防ぐ有効な手法です。訓練時にランダムにニューロンを無効化することで、モデルが特定のニューロンに過度に依存することを防ぎ、汎化性能を向上させます。</li><li><strong>B) 早期停止（Early Stopping）:</strong> 早期停止は過学習を防ぐ有効な手法です。検証データセットの性能をモニタリングし、検証損失が増加し始めたら訓練を停止することで、過学習を防ぎます。</li><li><strong>D) 正則化（Regularization）:</strong> 正則化（L1/L2正則化など）は過学習を防ぐ有効な手法です。損失関数に重みの大きさに対するペナルティを追加することで、モデルの複雑さを制限し、過学習を防ぎます。</li></ul>",
  "resources": []
}