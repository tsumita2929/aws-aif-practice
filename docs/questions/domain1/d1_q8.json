{
  "id": "d1_q8",
  "type": "single",
  "text": "ニューラルネットワークの「過学習（オーバーフィッティング）」を防ぐ手法として適切でないものはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ドロップアウト"
    },
    {
      "label": "B",
      "text": "早期停止（Early Stopping）"
    },
    {
      "label": "C",
      "text": "学習率の増加"
    },
    {
      "label": "D",
      "text": "正則化（Regularization）"
    }
  ],
  "correct": [
    2
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>過学習は、モデルが訓練データに過度に適合し、新しいデータに対する汎化性能が低下する現象です。学習率の増加は過学習を悪化させる可能性があります。</p>\n                \n                <h5>過学習を防ぐ主な手法</h5>\n                <ul>\n                    <li><strong>ドロップアウト</strong>: 訓練時にランダムにニューロンを無効化し、モデルの複雑さを制御</li>\n                    <li><strong>早期停止</strong>: 検証データの性能が悪化し始めたら学習を停止</li>\n                    <li><strong>正則化</strong>: L1/L2正則化により重みの大きさにペナルティを課す</li>\n                    <li><strong>データ拡張</strong>: 訓練データを人工的に増やす</li>\n                </ul>\n                \n                <h5>なぜ学習率の増加が適切でないのか</h5>\n                <ul>\n                    <li>高い学習率は最適解を飛び越えてしまう可能性がある</li>\n                    <li>不安定な学習により過学習のリスクが増大</li>\n                    <li>通常は学習率の減衰（decay）を使用して安定した収束を図る</li>\n                </ul>\n                \n                <h5>実務でのベストプラクティス</h5>\n                <p>過学習の兆候（訓練誤差は下がるが検証誤差が上昇）を監視し、複数の手法を組み合わせて対処することが重要です。</p>\n            ",
  "resources": []
}