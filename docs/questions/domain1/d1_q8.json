{
  "id": "d1_q8",
  "type": "single",
  "text": "ニューラルネットワークの「過学習（オーバーフィッティング）」を防ぐ手法として適切でないものはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ドロップアウト"
    },
    {
      "label": "B",
      "text": "早期停止（Early Stopping）"
    },
    {
      "label": "C",
      "text": "学習率の増加"
    },
    {
      "label": "D",
      "text": "正則化（Regularization）"
    }
  ],
  "correct": [
    2
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>過学習は、モデルが訓練データに過度に適合し、新しいデータに対する汎化性能が低下する現象です。学習率の増加は過学習を悪化させる可能性があります。</p>\n                \n                <h5>過学習を防ぐ主な手法</h5>\n                <ul>\n                    <li><strong>ドロップアウト</strong>: 訓練時にランダムにニューロンを無効化し、モデルの複雑さを制御</li>\n                    <li><strong>早期停止</strong>: 検証データの性能が悪化し始めたら学習を停止</li>\n                    <li><strong>正則化</strong>: L1/L2正則化により重みの大きさにペナルティを課す</li>\n                    <li><strong>データ拡張</strong>: 訓練データを人工的に増やす</li>\n                </ul>\n                \n                <h5>なぜ学習率の増加が適切でないのか</h5>\n                <ul>\n                    <li>高い学習率は最適解を飛び越えてしまう可能性がある</li>\n                    <li>不安定な学習により過学習のリスクが増大</li>\n                    <li>通常は学習率の減衰（decay）を使用して安定した収束を図る</li>\n                </ul>\n                \n                <h5>実務でのベストプラクティス</h5>\n                <p>過学習の兆候（訓練誤差は下がるが検証誤差が上昇）を監視し、複数の手法を組み合わせて対処することが重要です。</p>\n            <h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) ドロップアウト:</strong> ドロップアウトは過学習を防ぐ効果的な手法です。訓練時にランダムにニューロンを無効化（ドロップ）することで、ネットワークが特定のニューロンに過度に依存することを防ぎ、より頑健なモデルを構築します。</li><li><strong>B) 早期停止（Early Stopping）:</strong> 早期停止は過学習を防ぐ実用的な手法です。検証データの損失をモニタリングし、改善が見られなくなったら訓練を停止することで、訓練データへの過剰な適合を防ぎます。</li><li><strong>D) 正則化（Regularization）:</strong> 正則化（L1/L2正則化）は過学習を防ぐ基本的な手法です。損失関数に重みの大きさに対するペナルティ項を追加することで、モデルの複雑さを制限し、汎化性能を向上させます。</li></ul>",
  "resources": []
}