{
  "id": "d1_q18",
  "type": "single",
  "text": "勾配降下法（Gradient Descent）において、学習率（Learning Rate）が大きすぎる場合に起こる問題は何ですか？",
  "choices": [
    {
      "label": "A",
      "text": "収束が遅くなる"
    },
    {
      "label": "B",
      "text": "最適解を飛び越えて収束しない可能性がある"
    },
    {
      "label": "C",
      "text": "過学習が防げる"
    },
    {
      "label": "D",
      "text": "計算時間が短縮される"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>勾配降下法において、学習率が大きすぎると、パラメータの更新幅が大きくなりすぎて、最適解を飛び越えてしまい、収束しない、または発散する可能性があります。</p>\n                \n                <h5>学習率の影響</h5>\n                <ul>\n                    <li><strong>学習率が大きすぎる場合</strong>: 最適解を飛び越え、振動または発散</li>\n                    <li><strong>学習率が適切な場合</strong>: 安定して最適解に収束</li>\n                    <li><strong>学習率が小さすぎる場合</strong>: 収束は安定するが、非常に遅い</li>\n                </ul>\n                \n                <h5>学習率の調整手法</h5>\n                <ul>\n                    <li><strong>固定学習率</strong>: 単純だが最適値の設定が困難</li>\n                    <li><strong>学習率減衰</strong>: エポックごとに学習率を減少</li>\n                    <li><strong>適応的学習率</strong>: Adam、RMSpropなど、パラメータごとに学習率を自動調整</li>\n                    <li><strong>学習率スケジューリング</strong>: Cosine Annealing、Warm-up など</li>\n                </ul>\n                \n                <h5>なぜ他の選択肢が誤りなのか</h5>\n                <ul>\n                    <li><strong>選択肢A</strong>: 学習率が大きいと収束が速くなる可能性もあるが、不安定</li>\n                    <li><strong>選択肢C</strong>: 過学習とは関係なく、最適化の収束性の問題</li>\n                    <li><strong>選択肢D</strong>: 計算時間は短くなる可能性があるが、収束しない</li>\n                </ul>\n                \n                <h5>実務でのベストプラクティス</h5>\n                <p>初期学習率を複数試し、学習曲線を観察。一般的には0.001〜0.1の範囲から開始し、Adamなどの適応的最適化手法を使用。学習の進行に応じて学習率を減衰させることも効果的です。</p>\n            ",
  "resources": []
}