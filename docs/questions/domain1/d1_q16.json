{
  "id": "d1_q16",
  "type": "single",
  "text": "機械学習における「次元の呪い」とは何を指しますか？",
  "choices": [
    {
      "label": "A",
      "text": "モデルの学習時間が短すぎること"
    },
    {
      "label": "B",
      "text": "特徴量の次元数が増えると、必要なデータ量が指数関数的に増加する現象"
    },
    {
      "label": "C",
      "text": "データの前処理が不要になること"
    },
    {
      "label": "D",
      "text": "モデルの精度が常に向上すること"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>「次元の呪い」とは、特徴量の次元数が増加すると、データが高次元空間で疎になり、モデルの学習に必要なデータ量が指数関数的に増加する現象です。</p>\n                \n                <h5>次元の呪いが引き起こす問題</h5>\n                <ul>\n                    <li><strong>データの疎性</strong>: 高次元空間ではデータ点間の距離が均一化し、類似性の判断が困難</li>\n                    <li><strong>過学習リスク</strong>: パラメータ数に対してデータが不足し、過学習しやすくなる</li>\n                    <li><strong>計算コストの増大</strong>: 次元数に比例して計算量が増加</li>\n                    <li><strong>可視化の困難さ</strong>: 3次元以上のデータは直感的な理解が困難</li>\n                </ul>\n                \n                <h5>なぜ他の選択肢が誤りなのか</h5>\n                <ul>\n                    <li><strong>選択肢A</strong>: 学習時間は増加する傾向にあり、短くなることはない</li>\n                    <li><strong>選択肢C</strong>: 前処理の重要性はむしろ増加する</li>\n                    <li><strong>選択肢D</strong>: 精度は低下する可能性が高い</li>\n                </ul>\n                \n                <h5>次元削減の手法</h5>\n                <ul>\n                    <li><strong>主成分分析（PCA）</strong>: 分散を最大化する方向に次元を削減</li>\n                    <li><strong>特徴選択</strong>: 重要な特徴のみを選択</li>\n                    <li><strong>オートエンコーダー</strong>: ニューラルネットワークで次元圧縮</li>\n                    <li><strong>t-SNE/UMAP</strong>: 高次元データの可視化に特化</li>\n                </ul>\n                \n                <h5>実務での対策</h5>\n                <p>特徴量が多い場合は、まず相関分析や重要度評価を行い、不要な特徴を削除。その後、PCAなどで次元削減を行うことが一般的です。</p>\n            ",
  "resources": []
}