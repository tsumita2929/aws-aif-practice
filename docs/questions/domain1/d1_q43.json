{
  "id": "d1_q43",
  "type": "single",
  "text": "「確率的勾配降下法（SGD）」と「バッチ勾配降下法」の主な違いは何ですか？",
  "choices": [
    {
      "label": "A",
      "text": "SGDは全データを使用し、バッチは一部のみ使用"
    },
    {
      "label": "B",
      "text": "SGDは一つまたは少数のサンプルで重みを更新、バッチは全データで更新"
    },
    {
      "label": "C",
      "text": "両者に違いはない"
    },
    {
      "label": "D",
      "text": "SGDは遅く、バッチは速い"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>確率的勾配降下法（SGD）とバッチ勾配降下法は、ニューラルネットワークの重み更新方法の違いです。SGDは少数のサンプルで頻繁に更新し、バッチ勾配降下法は全データで更新します。</p>\n                \n                <h5>各手法の特徴</h5>\n                <table border=\"1\" style=\"width: 100%; margin: 10px 0;\">\n                    <tr>\n                        <th>項目</th>\n                        <th>バッチ勾配降下法</th>\n                        <th>確率的勾配降下法（SGD）</th>\n                        <th>ミニバッチSGD</th>\n                    </tr>\n                    <tr>\n                        <td>更新頻度</td>\n                        <td>エポックごと（全データ後）</td>\n                        <td>サンプルごと</td>\n                        <td>ミニバッチごと</td>\n                    </tr>\n                    <tr>\n                        <td>使用データ量</td>\n                        <td>全データ</td>\n                        <td>1サンプル</td>\n                        <td>32-256サンプル程度</td>\n                    </tr>\n                    <tr>\n                        <td>収束性</td>\n                        <td>安定的、滑らか</td>\n                        <td>振動が大きい</td>\n                        <td>比較的安定</td>\n                    </tr>\n                    <tr>\n                        <td>計算効率</td>\n                        <td>GPUで効率的</td>\n                        <td>並列化困難</td>\n                        <td>GPUで最も効率的</td>\n                    </tr>\n                    <tr>\n                        <td>メモリ使用量</td>\n                        <td>大（全データ）</td>\n                        <td>小（1サンプル）</td>\n                        <td>中程度</td>\n                    </tr>\n                </table>\n                \n                <h5>なぜ他の選択肢が誤りなのか</h5>\n                <ul>\n                    <li><strong>A：</strong> 逆です。バッチが全データ、SGDが一部のデータを使用</li>\n                    <li><strong>C：</strong> 明確な違いがあり、それぞれ異なる特性を持つ</li>\n                    <li><strong>D：</strong> 1エポックあたりの時間はSGDの方が速い（更新回数は多いが）</li>\n                </ul>\n                \n                <h5>SGDの利点と欠点</h5>\n                <p><strong>利点：</strong><br>\n                ・大規模データセットでも高速に学習開始<br>\n                ・局所最適解から脱出しやすい（ノイズによる）<br>\n                ・オンライン学習が可能<br>\n                ・メモリ効率が良い</p>\n                \n                <p><strong>欠点：</strong><br>\n                ・収束が不安定（学習率の調整が重要）<br>\n                ・並列化が困難<br>\n                ・最終的な収束に時間がかかる場合がある</p>\n                \n                <h5>実装例の比較</h5>\n                <pre><code># バッチ勾配降下法\nfor epoch in range(num_epochs):\n    # 全データで勾配計算\n    gradients = compute_gradients(X_all, y_all, weights)\n    weights -= learning_rate * gradients\n\n# 確率的勾配降下法（SGD）\nfor epoch in range(num_epochs):\n    for x_i, y_i in zip(X, y):\n        # 1サンプルで勾配計算\n        gradient = compute_gradient(x_i, y_i, weights)\n        weights -= learning_rate * gradient\n\n# ミニバッチSGD（最も一般的）\nfor epoch in range(num_epochs):\n    for batch in get_minibatches(X, y, batch_size=32):\n        gradients = compute_gradients(batch.X, batch.y, weights)\n        weights -= learning_rate * gradients</code></pre>\n                \n                <h5>実務での使い分け</h5>\n                <ul>\n                    <li><strong>小規模データ：</strong> バッチ勾配降下法で安定的な学習</li>\n                    <li><strong>大規模データ：</strong> ミニバッチSGDでメモリと速度のバランス</li>\n                    <li><strong>オンライン学習：</strong> SGDで逐次的にモデル更新</li>\n                    <li><strong>現代の深層学習：</strong> AdamやRMSpropなどの適応的学習率を持つSGD派生手法</li>\n                </ul>\n            ",
  "resources": []
}