{
  "id": "d1_q37",
  "type": "single",
  "text": "「勾配消失問題」を緩和する手法として適切でないものはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "ReLU活性化関数の使用"
    },
    {
      "label": "B",
      "text": "バッチ正規化"
    },
    {
      "label": "C",
      "text": "残差接続（ResNet）"
    },
    {
      "label": "D",
      "text": "シグモイド関数を全層で使用"
    }
  ],
  "correct": [
    3
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>勾配消失問題は、深いニューラルネットワークで勾配が層を逆伝播する際に指数関数的に小さくなり、学習が停滞する現象です。シグモイド関数は、この問題を悪化させる代表的な活性化関数です。</p>\n                \n                <h5>勾配消失問題の原因</h5>\n                <ul>\n                    <li><strong>シグモイド関数の問題点：</strong>\n                        <ul>\n                            <li>出力範囲：0～1</li>\n                            <li>微分の最大値：0.25</li>\n                            <li>深い層では勾配が 0.25^n のように減衰</li>\n                            <li>飽和領域（0付近、1付近）で勾配がほぼ0</li>\n                        </ul>\n                    </li>\n                    <li><strong>連鎖律の影響：</strong>\n                        <ul>\n                            <li>逆伝播で勾配が掛け算される</li>\n                            <li>小さい値の連続的な掛け算で急速に減衰</li>\n                        </ul>\n                    </li>\n                </ul>\n                \n                <h5>効果的な緩和手法</h5>\n                <ul>\n                    <li><strong>A. ReLU活性化関数：</strong>\n                        <ul>\n                            <li>f(x) = max(0, x)</li>\n                            <li>正の領域で勾配が1で一定</li>\n                            <li>勾配消失を大幅に軽減</li>\n                            <li>計算も高速</li>\n                        </ul>\n                    </li>\n                    <li><strong>B. バッチ正規化：</strong>\n                        <ul>\n                            <li>各層の入力を正規化</li>\n                            <li>内部共変量シフトを軽減</li>\n                            <li>より大きな学習率が使用可能</li>\n                            <li>勾配の流れを改善</li>\n                        </ul>\n                    </li>\n                    <li><strong>C. 残差接続（ResNet）：</strong>\n                        <ul>\n                            <li>スキップ接続により勾配が直接伝播</li>\n                            <li>恒等写像の学習が容易</li>\n                            <li>非常に深いネットワークが可能</li>\n                        </ul>\n                    </li>\n                </ul>\n                \n                <h5>その他の対策</h5>\n                <ul>\n                    <li><strong>適切な重み初期化：</strong>\n                        <ul>\n                            <li>He初期化（ReLU用）</li>\n                            <li>Xavier/Glorot初期化</li>\n                        </ul>\n                    </li>\n                    <li><strong>改良された活性化関数：</strong>\n                        <ul>\n                            <li>Leaky ReLU</li>\n                            <li>ELU、SELU</li>\n                            <li>Swish、GELU</li>\n                        </ul>\n                    </li>\n                </ul>\n                \n                <h5>実装例</h5>\n                <pre><code># PyTorchでの実装\nimport torch.nn as nn\n\nclass ImprovedNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3)\n        self.bn1 = nn.BatchNorm2d(64)  # バッチ正規化\n        self.relu = nn.ReLU()           # ReLU活性化\n        \n    def forward(self, x):\n        identity = x  # 残差接続用\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = x + identity  # 残差接続\n        return x</code></pre>\n                \n                <h5>なぜシグモイド関数が問題か</h5>\n                <p>10層のネットワークでシグモイドを使用した場合：</p>\n                <ul>\n                    <li>勾配の上限：0.25^10 ≈ 9.5×10^-7</li>\n                    <li>実質的に学習が停止</li>\n                    <li>深い層のパラメータが更新されない</li>\n                </ul>\n            <h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) ReLU活性化関数の使用:</strong> ReLUは勾配消失問題を緩和する効果的な手法です。正の領域で勾配が1で一定のため、深い層でも勾配が消失しにくくなります。</li><li><strong>B) バッチ正規化:</strong> これも勾配消失を緩和する有効な手法です。各層の入力を正規化することで、勾配の流れを改善し、より深いネットワークの学習を可能にします。</li><li><strong>C) 残差接続（ResNet）:</strong> 残差接続は勾配消失問題の画期的な解決策です。スキップ接続により勾配が直接伝播するため、非常に深いネットワークでも学習可能になります。</li></ul>",
  "resources": []
}