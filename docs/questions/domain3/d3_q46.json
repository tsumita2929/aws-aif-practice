{
  "id": "d3_q46",
  "type": "single",
  "text": "生成AIモデルの「ハルシネーション」（幻覚）を軽減するための最も効果的なアプローチはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "より大きなモデルを使用する"
    },
    {
      "label": "B",
      "text": "RAGアーキテクチャと事実確認メカニズムの実装"
    },
    {
      "label": "C",
      "text": "プロンプトを短くする"
    },
    {
      "label": "D",
      "text": "出力の文字数を制限する"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "<h5>詳細解説</h5><p>正解はB「RAGアーキテクチャと事実確認メカニズムの実装」です。</p><p>生成AIモデルの「ハルシネーション」（幻覚）は、モデルが事実と異なる情報や存在しない情報を生成する現象で、生成AIの信頼性における重大な課題です。</p><h5>RAG（Retrieval-Augmented Generation）アーキテクチャと事実確認メカニズムが効果的な理由</h5><h5>1. RAGアーキテクチャの利点</h5><ul><li>外部知識ベースからの情報検索により、事実に基づいた生成が可能</li><li>最新情報へのアクセスによる時代遅れの情報の回避</li><li>ソース情報の追跡可能性</li><li>ドメイン特化型知識の活用</li></ul><h5>2. 事実確認メカニズム</h5><ul><li>生成された内容の検証プロセス</li><li>信頼できるソースとの照合</li><li>矛盾検出システム</li><li>信頼度スコアの算出</li></ul><h5>3. 実装方法</h5><ul><li>ベクトルデータベースの活用</li><li>知識グラフとの統合</li><li>マルチステップ検証</li><li>ヒューマン・イン・ザ・ループの組み込み</li></ul><h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) より大きなモデルを使用する:</strong> モデルサイズの増大だけではハルシネーションを解決できず、むしろより説得力のある虚偽情報を生成する可能性があります。</li><li><strong>C) プロンプトを短くする:</strong> プロンプトの長さはハルシネーションの根本原因ではなく、短くしても問題は解決しません。</li><li><strong>D) 出力の文字数を制限する:</strong> 文字数制限はハルシネーションの量を減らすかもしれませんが、発生自体を防ぐことはできません。</li></ul><p>ハルシネーションの軽減は、技術的対策と運用上の工夫の組み合わせが重要です。</p>",
  "resources": []
}