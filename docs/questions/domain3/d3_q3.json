{
  "id": "d3_q3",
  "type": "single",
  "text": "大手テクノロジー企業の採用AIシステムが、技術職の候補者推薦において男性を75%、女性を25%の割合で推薦していることが判明しました。調査の結果、訓練データの男女比が80:20であったことが分かりました。この状況に対する最も包括的な対応策はどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "Adversarial Debiasing技術を使用し、性別情報を除外しながらモデルを再訓練する"
    },
    {
      "label": "B",
      "text": "訓練データの男女比を50:50に調整し、公平性制約（Demographic Parity）を追加してモデルを再構築する"
    },
    {
      "label": "C",
      "text": "性別属性を完全に削除し、他の属性のみでモデルを訓練する（ただし、代理変数の影響は考慮しない）"
    },
    {
      "label": "D",
      "text": "推論時に事後的に結果を調整し、男女比を50:50にする"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "\n                <h5>詳細解説</h5>\n                <p>データの偏りを修正し、公平性制約を追加することで、システマティックなバイアスに対処できます。</p>\n                <h5>なぜBが最適か</h5>\n                <ul>\n                    <li>データレベルでの根本的な対処</li>\n                    <li>公平性制約により、アルゴリズムレベルでも保証</li>\n                    <li>透明性のある手法</li>\n                </ul>\n                <h5>他の選択肢の問題点</h5>\n                <ul>\n                    <li><strong>A:</strong> 性別情報の除外だけでは代理変数を通じたバイアスが残る</li>\n                    <li><strong>C:</strong> 代理変数（学歴、職歴など）を通じた間接的差別の可能性</li>\n                    <li><strong>D:</strong> 事後調整は根本解決にならず、新たな不公平を生む可能性</li>\n                </ul>\n            <h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) Adversarial Debiasing技術を使用し、性別情報を除外しながらモデルを再訓練する:</strong> 性別情報の除外だけでは代理変数を通じたバイアスが残る</li><li><strong>C) 性別属性を完全に削除し、他の属性のみでモデルを訓練する（ただし、代理変数の影響は考慮しない）:</strong> 代理変数（学歴、職歴など）を通じた間接的差別の可能性</li><li><strong>D) 推論時に事後的に結果を調整し、男女比を50:50にする:</strong> 事後調整は根本解決にならず、新たな不公平を生む可能性</li></ul>",
  "resources": [
    {
      "title": "AWS AI Service Cards",
      "url": "https://aws.amazon.com/machine-learning/responsible-ai/"
    }
  ]
}