{
  "id": "d3_q10",
  "type": "single",
  "text": "次のシナリオを考えてください： 「採用プロセスでAIを使用する企業が、過去10年間の採用データでモデルを訓練した結果、特定の性別に偏った推薦をするようになった」 この問題を解決する最も適切なアプローチはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "より高性能なハードウェアを使用する"
    },
    {
      "label": "B",
      "text": "データの偏りを分析し、バランスの取れたデータセットで再訓練する"
    },
    {
      "label": "C",
      "text": "AIの使用を完全に中止する"
    },
    {
      "label": "D",
      "text": "プログラミング言語を変更する"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "正解はB「データの偏りを分析し、バランスの取れたデータセットで再訓練する」です。\n\nこのシナリオは、過去の採用データに存在する性別バイアスがAIモデルに学習されてしまった典型的な例です。過去10年間のデータに特定の性別への偏りがあった場合（例：技術職で男性の採用が多かった）、AIモデルもその偏りを再現してしまいます。\n\n解決アプローチ：\n1. 既存データの偏りを統計的に分析し、どのような偏りが存在するかを特定\n2. データの再サンプリング、合成データの生成、重み付けなどの技術を使用してバランスの取れたデータセットを作成\n3. 公平性制約を含むアルゴリズムで再訓練\n4. 複数の公平性指標を使用して結果を評価\n\n他の選択肢が不適切な理由：\n- A「より高性能なハードウェアを使用する」：ハードウェアの性能はバイアスの問題を解決しません。\n- C「AIの使用を完全に中止する」：問題を解決するのではなく、技術の利点を放棄することになります。\n- D「プログラミング言語を変更する」：プログラミング言語はバイアスとは無関係です。",
  "resources": []
}