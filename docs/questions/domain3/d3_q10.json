{
  "id": "d3_q10",
  "type": "single",
  "text": "次のシナリオを考えてください： 「採用プロセスでAIを使用する企業が、過去10年間の採用データでモデルを訓練した結果、特定の性別に偏った推薦をするようになった」 この問題を解決する最も適切なアプローチはどれですか？",
  "choices": [
    {
      "label": "A",
      "text": "より高性能なハードウェアを使用する"
    },
    {
      "label": "B",
      "text": "データの偏りを分析し、バランスの取れたデータセットで再訓練する"
    },
    {
      "label": "C",
      "text": "AIの使用を完全に中止する"
    },
    {
      "label": "D",
      "text": "プログラミング言語を変更する"
    }
  ],
  "correct": [
    1
  ],
  "explanation": "<h5>詳細解説</h5><p>正解はB「データの偏りを分析し、バランスの取れたデータセットで再訓練する」です。</p><p>このシナリオは、過去の採用データに存在する性別バイアスがAIモデルに学習されてしまった典型的な例です。過去10年間のデータに特定の性別への偏りがあった場合（例：技術職で男性の採用が多かった）、AIモデルもその偏りを再現してしまいます。</p><h5>解決アプローチ</h5><ol><li>既存データの偏りを統計的に分析し、どのような偏りが存在するかを特定</li><li>データの再サンプリング、合成データの生成、重み付けなどの技術を使用してバランスの取れたデータセットを作成</li><li>公平性制約を含むアルゴリズムで再訓練</li><li>複数の公平性指標を使用して結果を評価</li></ol><h5>なぜ他の選択肢が間違っているのか</h5><ul><li><strong>A) より高性能なハードウェアを使用する:</strong> ハードウェアの性能はバイアスの問題を解決しません。</li><li><strong>C) AIの使用を完全に中止する:</strong> 問題を解決するのではなく、技術の利点を放棄することになります。</li><li><strong>D) プログラミング言語を変更する:</strong> プログラミング言語はバイアスとは無関係です。</li></ul>",
  "resources": []
}